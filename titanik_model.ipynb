{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразование напрямую в датафрейм библиотеки Pandas\n",
    "\n",
    "# импортируем библиотеку\n",
    "import pandas as pd\n",
    "\n",
    "# применим функцию read_csv() и посмотрим на первые три записи файла train.csv\n",
    "train = pd.read_csv('/content/train.csv')\n",
    "train.head(3)\n",
    "# сделаем то же самое с файлом test.csv\n",
    "test = pd.read_csv('/content/test.csv')\n",
    "test.head(3)\n",
    "\n",
    "train.info()\n",
    "\n",
    "# для построения графиков воспользуемся новой для нас библиотекой seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# посмотрим насколько значим класс билета для выживания пассажира\n",
    "# с помощью x и hue мы можем уместить две категориальные переменные на одном графике\n",
    "sns.countplot(x = 'Pclass', hue = 'Survived', data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кто выживал чаще, мужчины или женщины?\n",
    "sns.countplot(x = 'Sex', hue = 'Survived', data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выявим пропущенные значения с помощью .isnull() и посчитаем их количество sum()\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переменная Cabin (номер каюты), скорее всего, не является самой важной\n",
    "# избавимся от нее с помощью метода .drop()\n",
    "# (параметр axis отвечает за столбцы, inplace = True сохраняет изменения)\n",
    "train.drop(columns = 'Cabin', axis = 1, inplace = True)\n",
    "# а вот Age (возраст) точно важен, заменим пустые значения средним арифметическим\n",
    "train['Age'].fillna(train['Age'].mean(), inplace = True)\n",
    "# у нас остаются две пустые строки в Embarked, удалим их\n",
    "train.dropna(inplace = True)\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применим one-hot encoding к переменной Sex (пол) с помощью метода .get_dummies()\n",
    "pd.get_dummies(train['Sex']).head(3)\n",
    "# удалим первый столбец, он избыточен\n",
    "sex = pd.get_dummies(train['Sex'], drop_first = True)\n",
    "sex.head(3)\n",
    "\n",
    "# Сделаем то же самое для переменных Pclass и Embarked\n",
    "embarked = pd.get_dummies(train['Embarked'], drop_first = True)\n",
    "pclass = pd.get_dummies(train['Pclass'], drop_first = True)\n",
    "\n",
    "# Присоединим новые (закодированные) переменные к исходному датафрейму train.\n",
    "train = pd.concat([train, pclass, sex, embarked], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отбор признаков\n",
    "\n",
    "# В первую очередь, удалим исходные (до применения one-hot encoding)\n",
    "# переменные Sex, Pclass и Embarked\n",
    "# Кроме того, переменные PassengerId, Name и Ticket вряд ли скажут что-то\n",
    "# определенное о шансах на выживание пассажира, удалим и их\n",
    "\n",
    "# применим функцию drop() к соответствующим столбцам\n",
    "train.drop(['PassengerId', 'Pclass', 'Name', 'Sex', 'Ticket', 'Embarked'], axis = 1, inplace = True)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация данных\n",
    "\n",
    "# импортируем класс StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# создадим объект этого класса\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# выберем те столбцы, которые мы хотим масштабировать\n",
    "cols_to_scale = ['Age', 'Fare']\n",
    "\n",
    "# рассчитаем среднее арифметическое и СКО для масштабирования данных\n",
    "scaler.fit(train[cols_to_scale])\n",
    "\n",
    "# применим их\n",
    "train[cols_to_scale] = scaler.transform(train[cols_to_scale])\n",
    "\n",
    "# посмотрим на результат\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переменные 2 и 3 (второй и третий класс) выражены числами, а не строками\n",
    "\n",
    "train.columns\n",
    "train.columns = train.columns.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение обучающей выборки на признаки и целевую переменную\n",
    "# поместим в X_train все кроме столбца Survived\n",
    "X_train = train.drop('Survived', axis = 1)\n",
    "\n",
    "# столбец 'Survived' станет нашей целевой переменной (y_train)\n",
    "y_train = train['Survived']\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели логистической регрессии\n",
    "\n",
    "# импортируем логистическую регрессию из модуля linear_model библиотеки sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# создадим объект этого класса и запишем его в переменную model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# обучим нашу модель\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем предсказание класса на обучающей выборке\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# построим матрицу ошибок\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# передадим ей фактические и прогнозные значения\n",
    "conf_matrix = confusion_matrix(y_train, y_pred_train)\n",
    "\n",
    "# преобразуем в датафрейм\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix)\n",
    "conf_matrix_df\n",
    "\n",
    "# Для удобства интерпретации добавим подписи.\n",
    "conf_matrix_labels = pd.DataFrame(conf_matrix, columns = ['Прогноз погиб', 'Прогноз выжил'], index = ['Факт погиб', 'Факт выжил'])\n",
    "conf_matrix_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"посмотрим на метрику accuracy. Она показывает долю правильно предсказанных значений. То есть мы берем тех, кого верно предсказали как погибших (true negative, TN, таких было 478), и тех, кого верно предсказали как выживших (true positive, TP, 237), и делим на общее число прогнозов.\"\"\"\n",
    "\n",
    "# рассчитаем метрику accuracy вручную\n",
    "round((478 + 237)/(478 + 237 + 71 + 103), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем метрику accuracy из sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# так же передадим ей фактические и прогнозные значения\n",
    "model_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "# округлим до трех знаков после запятой\n",
    "round(model_accuracy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение прогноза на тестовых данных\n",
    "\n",
    "test.info()\n",
    "\n",
    "\"\"\"Взглянув на сводку по тестовым данным, становится заметна одна сложность. Мы обучили модель на обработанных данных. В частности, мы заполнили пропуски, закодировали категориальные переменные и убрали лишние признаки. Кроме того, мы масштабировали количественные переменные и превратили названия столбцов в строки.\n",
    "\n",
    "Для того чтобы наша модель смогла работать с тестовой выборкой нам нужно таким же образом обработать и эти данные.\n",
    "При надо обратить внимание, мы не нарушаем принципа разделения данных, поскольку меняем тестовую выборку так же, как мы меняли обучающую.\"\"\"\n",
    "\n",
    "# для начала дадим датасету привычное название X_test\n",
    "X_test = test\n",
    "# заполним пропуски в переменных Age и Fare средним арифметическим\n",
    "X_test['Age'].fillna(test['Age'].mean(), inplace = True)\n",
    "X_test['Fare'].fillna(test['Fare'].mean(), inplace = True)\n",
    "\n",
    "# выполним one-hot encoding категориальных переменных\n",
    "sex = pd.get_dummies(X_test['Sex'], drop_first = True)\n",
    "embarked = pd.get_dummies(X_test['Embarked'], drop_first = True)\n",
    "pclass = pd.get_dummies(X_test['Pclass'], drop_first = True)\n",
    "\n",
    "# присоединим новые столбцы к исходному датафрейму\n",
    "X_test = pd.concat([test, pclass, sex, embarked], axis = 1)\n",
    "\n",
    "# и удалим данные, которые теперь не нужны\n",
    "X_test.drop(['PassengerId', 'Pclass', 'Name', 'Sex', 'Cabin', 'Ticket', 'Embarked'], axis = 1, inplace = True)\n",
    "\n",
    "# посмотрим на результат\n",
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Теперь нужно масштабировать количественные переменные. Для этого мы будем использовать те параметры (среднее арифметическое и СКО), которые мы получили при обработке обучающей выборки. Так мы сохраним единообразие изменений и избежим утечки данных (data leakage).\"\"\"\n",
    "\n",
    "# применим среднее арифметическое и СКО обучающей выборки для масштабирования тестовых данных\n",
    "X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "X_test.head(3)\n",
    "\n",
    "# Превращаем название столбцов в строки\n",
    "X_test.columns = X_test.columns.map(str)\n",
    "\n",
    "# Делаем прогноз на тестовой выборке\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# посмотрим на первые 10 прогнозных значений\n",
    "y_pred_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение результата в новом файле на сервере\n",
    "\n",
    "# возьмем индекс пассажиров из столбца PassengerId тестовой выборки\n",
    "ids = test['PassengerId']\n",
    " \n",
    "# создадим датафрейм из словаря, в котором\n",
    "# первая пара ключа и значения - это id пассажира, вторая - прогноз \"на тесте\"\n",
    "result = pd.DataFrame({'PassengerId': ids, 'Survived': y_pred_test})\n",
    " \n",
    "# посмотрим, что получилось\n",
    "result.head()\n",
    "# создадим новый файл result.csv с помощью функции to_csv(), удалив при этом индекс\n",
    "result.to_csv('result.csv', index = False)\n",
    " \n",
    "# файл будет сохранен в 'Сессионном хранилище' и, если все пройдет успешно, выведем следующий текст:\n",
    "print('Файл успешно сохранился в сессионное хранилище!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скачать файл на жесткий диск.\n",
    "# применим метод .download() объекта files\n",
    "from google.colab import files\n",
    "files.download('/content/result.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
